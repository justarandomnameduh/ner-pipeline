{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Named Entity Recognition Pipeline and demonstrated with 3 different NLP libraries - NLTK, Stanza, SpaCy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Named Entity Recognition (NER)?\n",
    "\n",
    "**Named Entity Recognition (NER)** is a key technique in natural language processing aimed at extracting specific information from unstructured text. It is particularly valuable in scenarios where manually reading and annotating large volumes of documents is impractical.\n",
    "\n",
    "A **named entity** refers to any real-world object or concept that has a proper name or label. These entities can vary widely depending on the domain but often include categories like:\n",
    "\n",
    "- People\n",
    "- Organizations\n",
    "- Locations\n",
    "- Countries\n",
    "- Languages\n",
    "- Works of art\n",
    "- Dates and times\n",
    "- Numerical data, such as quantities and measurements\n",
    "\n",
    "---\n",
    "\n",
    "In practice, entities can be tailored to the specific domain or dataset you are working with. For instance, if you are analyzing archaeological reports, you might define custom entities like \"culture,\" \"material,\" or \"method\" to extract more relevant insights.\n",
    "\n",
    "NER involves both identifying and classifying these entities within a text. Entities can be as simple as single tokens (e.g., \"Berlin\" as a location) or span multiple tokens (e.g., \"The Royal Society\" as an organization or \"Charles Robert Darwin\" as a person). This ability to handle varying structures makes NER a flexible and powerful tool for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, Named Entity parsers are implemented using models from three widely-used NLP libraries: [**spaCy**](https://spacy.io/), [**nltk**](https://www.nltk.org/), and [**stanza**](https://stanfordnlp.github.io/stanza/). Each library is well-documented, making it relatively straightforward to get named entity parsing up and running.\n",
    "\n",
    "To facilitate testing and debugging, a small sample of 10 sentences is provided, along with their corresponding gold labels. These are contained in two files: **sample.txt.gz** (input) and **sample.conllu.gz** (gold labels). These sample files highlight cases where incorrect tokenization could lead to misalignment. It is highly recommended to use these files during the initial development phase before running the full dataset, as processing the complete dataset may take up to an hour on a standard laptop for certain models.\n",
    "\n",
    "For convenience, code snippets to process the sample files line by line are included, along with information on the file formats. The headers and setup code required for downloading the necessary library models are also provided. These assume that you’ve set up an environment based on the included `requirements.txt` file.\n",
    "\n",
    "Additionally, a set of utility functions is provided to handle tasks like logging, writing output files, and converting label formats.\n",
    "\n",
    "This streamlined framework is designed to help you focus on implementing and comparing the models without worrying about peripheral setup issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT HEADERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import sys, os, gzip\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "import stanza\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\n",
    "from spacy.tokens import Doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL LOGFILE INITIALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move logfile to backup each time the kernel is reran\n",
    "backup_logfile = 'NER.log.bak'\n",
    "logfile = 'NER.log'\n",
    "if os.path.exists(backup_logfile):\n",
    "  os.remove(backup_logfile)\n",
    "if os.path.exists(logfile):\n",
    "  os.rename(logfile, backup_logfile)\n",
    "\n",
    "#Initialise logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='NER.log', \n",
    "                    level=logging.DEBUG)\n",
    "# Example logging output types you can use.\n",
    "# logger.debug('This debug message should go to the log file')\n",
    "# logger.info('Info should be this')\n",
    "# logger.warning('And this warning, too')\n",
    "# logger.error('And an error')\n",
    "# END GLOBAL LOGFILE INITIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models have been downloaded. To re-enable, remove the models.done file in your working directory.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have all of the models downloaded. You should only need to run this once.\n",
    "def get_all_models():\n",
    "  if not os.path.exists('models.done'):\n",
    "    print('Downloading necessary model files.')\n",
    "    logger.info(f'\\nDownloading necessary model files.\\n')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('maxent_ne_chunker')\n",
    "    nltk.download('words')\n",
    "    spacy.cli.download('en_core_web_sm')\n",
    "    spacy.cli.download('en_core_web_md')\n",
    "    spacy.cli.download('en_core_web_lg')\n",
    "    spacy.cli.download('en_core_web_trf')\n",
    "    open('models.done', 'w').close()\n",
    "  else:\n",
    "    print ('Models have been downloaded. To re-enable, remove the models.done file in your working directory.')\n",
    "    logger.info(f'\\nModels have been downloaded. To re-enable, remove the models.done file in your working directory.\\n')\n",
    "\n",
    "# If something fails, then remove the file 'models.done' in your working directory and the downloads will run again.\n",
    "get_all_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator function to compute function running time.\n",
    "def nlptimer(func):\n",
    "  \"\"\"Function decorator to measure execution time of functions.\"\"\"\n",
    "  def wrapper(*args, **kwargs):\n",
    "    start_time = time.time()  # Start time\n",
    "    result = func(*args, **kwargs)  # Execute the function\n",
    "    end_time = time.time()  # End time\n",
    "    execution_time = end_time - start_time  # Calculate execution time\n",
    "    print(f'\\n{func.__name__} executed in: {execution_time} seconds\\n')\n",
    "    logger.info(f'\\n{func.__name__} executed in: {execution_time} seconds\\n')\n",
    "    return result\n",
    "  return wrapper\n",
    "\n",
    "# Convert Ontonotes 5 formatted labels to conll03 format.\n",
    "@nlptimer\n",
    "def convert_ontonotes_to_conll(input_file, output_file):\n",
    "  # Mapping from OntoNotes 5 labels to CoNLL-2003 BIO labels\n",
    "  ontonotes_to_conll = {\n",
    "    \"B-PERSON\": \"B-PER\",\n",
    "    \"B-GPE\": \"B-LOC\",\n",
    "    \"B-LOC\": \"B-LOC\",\n",
    "    \"B-ORG\": \"B-ORG\",\n",
    "    \"B-NORP\": \"B-MISC\",\n",
    "    \"B-FAC\": \"B-MISC\",\n",
    "    \"B-PRODUCT\": \"B-MISC\",\n",
    "    \"B-EVENT\": \"B-MISC\",\n",
    "    \"B-WORK_OF_ART\": \"B-MISC\",\n",
    "    \"B-LAW\": \"B-MISC\",\n",
    "    \"B-LANGUAGE\": \"B-MISC\",\n",
    "    \"I-PERSON\": \"I-PER\",\n",
    "    \"I-GPE\": \"I-LOC\",\n",
    "    \"I-LOC\": \"I-LOC\",\n",
    "    \"I-ORG\": \"I-ORG\",\n",
    "    \"I-NORP\": \"I-MISC\",\n",
    "    \"I-FAC\": \"I-MISC\",\n",
    "    \"I-PRODUCT\": \"I-MISC\",\n",
    "    \"I-EVENT\": \"I-MISC\",\n",
    "    \"I-WORK_OF_ART\": \"I-MISC\",\n",
    "    \"I-LAW\": \"I-MISC\",\n",
    "    \"I-LANGUAGE\": \"I-MISC\",\n",
    "    # The following labels do not exist in CoNLL-2003 and are thus ignored\n",
    "    \"B-DATE\": \"O\",\n",
    "    \"B-TIME\": \"O\",\n",
    "    \"B-PERCENT\": \"O\",\n",
    "    \"B-MONEY\": \"O\",\n",
    "    \"B-QUANTITY\": \"O\",\n",
    "    \"B-ORDINAL\": \"O\",\n",
    "    \"B-CARDINAL\": \"O\",\n",
    "    \"I-DATE\": \"O\",\n",
    "    \"I-TIME\": \"O\",\n",
    "    \"I-PERCENT\": \"O\",\n",
    "    \"I-MONEY\": \"O\",\n",
    "    \"I-QUANTITY\": \"O\",\n",
    "    \"I-ORDINAL\": \"O\",\n",
    "    \"I-CARDINAL\": \"O\"\n",
    "  }\n",
    "\n",
    "  with gzip.open(input_file, 'rt') as infile, gzip.open(output_file, 'wt') as outfile:\n",
    "    for line in infile:\n",
    "      if line.strip() == '':\n",
    "        outfile.write('\\n')\n",
    "        continue\n",
    "      pos, token, ontonotes_label = line.strip().split('\\t')\n",
    "      conll_label = ontonotes_to_conll.get(ontonotes_label, None)\n",
    "      if conll_label:\n",
    "        outfile.write(f\"{pos}\\t{token}\\t{conll_label}\\n\")\n",
    "      else:\n",
    "        # If the label does not map to a CoNLL-2003 label, write the token with 'O'\n",
    "        outfile.write(f\"{pos}\\t{token}\\tO\\n\")\n",
    "  return\n",
    "\n",
    "# Convert NLTK formated labels to Conll03.\n",
    "@nlptimer\n",
    "def convert_nltk_to_conll(input_file, output_file):\n",
    "  # Mapping from OntoNotes 5 labels to CoNLL-2003 BIO labels\n",
    "  nltk_to_conll = {\n",
    "    \"B-FACILITY\": \"B-MISC\",\n",
    "    \"I-FACILITY\": \"I-MISC\",\n",
    "    \"B-GPE\": \"B-LOC\",\n",
    "    \"I-GPE\": \"I-LOC\",\n",
    "    \"B-GSP\": \"B-LOC\",\n",
    "    \"I-GSP\": \"I-LOC\",\n",
    "    \"B-LOCATION\": \"B-LOC\",\n",
    "    \"I-LOCATION\": \"I-LOC\",\n",
    "    \"B-ORGANIZATION\": \"B-ORG\",\n",
    "    \"I-ORGANIZATION\": \"I-ORG\",\n",
    "    \"B-PERSON\": \"B-PER\",\n",
    "    \"I-PERSON\": \"I-PER\",\n",
    "    \"I-FACILITY\": \"I-MISC\",\n",
    "    \"B-FACILITY\": \"B-MISC\"\n",
    "  }\n",
    "  with gzip.open(input_file, 'rt') as infile, gzip.open(output_file, 'wt') as outfile:\n",
    "    for line in infile:\n",
    "      if line.strip() == '':\n",
    "        outfile.write('\\n')\n",
    "        continue\n",
    "      pos, token, nltk_label = line.strip().split('\\t')\n",
    "      conll_label = nltk_to_conll.get(nltk_label, None)\n",
    "      if conll_label:\n",
    "        outfile.write(f\"{pos}\\t{token}\\t{conll_label}\\n\")\n",
    "      else:\n",
    "        # If the label does not map to a CoNLL-2003 label, write the token with 'O'\n",
    "        outfile.write(f\"{pos}\\t{token}\\tO\\n\")\n",
    "  return\n",
    "\n",
    "# Score a conllu03 formatted file.\n",
    "@nlptimer\n",
    "def score_conllu_file(input_file, gold_label_file):\n",
    "  # Since 'O' is the dominant label, and not a named entity, we will ignore\n",
    "  # tokens with this label since it would unbalance the classes being\n",
    "  # evaluated.\n",
    "  label_names = ['PER','MISC','LOC','ORG']\n",
    "  gold_labels = []\n",
    "  predicted_labels = []\n",
    "  with gzip.open(input_file, 'rt') as tst, gzip.open(gold_label_file, 'rt') as gold:\n",
    "    for tst_ln, gold_ln in zip(tst,gold):\n",
    "      tstfld = tst_ln.strip().split('\\t')\n",
    "      goldfld = gold_ln.strip().split('\\t')\n",
    "      # Verify that current token matches\n",
    "      # If test file and gold file are not aligned, exit with error.\n",
    "      if len(goldfld) == 3 and len(tstfld) == 3:\n",
    "        # Second item in both files should be the term that is tagged.\n",
    "        if tstfld[1] != goldfld[1]:\n",
    "          print('ERROR: Token {} does not match token {}'.format(tstfld[1],goldfld[1]))\n",
    "          logger.error('Token {} does not match token {}'.format(tstfld[1],goldfld[1]))\n",
    "          sys.exit(-1)\n",
    "        else:\n",
    "          t = ''\n",
    "          g = ''\n",
    "          if '-' in goldfld[2]:\n",
    "            gg = goldfld[2].split('-')\n",
    "            g = gg[1]\n",
    "          else:\n",
    "            g = goldfld[2]\n",
    "          if '-' in tstfld[2]:\n",
    "            tt = tstfld[2].split('-')\n",
    "            t = tt[1]\n",
    "          else:\n",
    "            g = tstfld[2]\n",
    "          # Since 'O' is the majority of labels, we will filter them out now.\n",
    "          if t != 'O' and g != 'O':\n",
    "            gold_labels.append(g)\n",
    "            predicted_labels.append(t)\n",
    "\n",
    "  # Now ensure that only valid labels are in the predicted labels.\n",
    "  # If this fails, you forgot to convert labels from ontonotes to conll03.\n",
    "  sanity_labels = ['PER','MISC','LOC','ORG', 'O']\n",
    "  for label in predicted_labels:\n",
    "    if label not in sanity_labels:\n",
    "      print('ERROR: Label {} not in valid label set {}.'.format(label,sanity_labels))\n",
    "      print('       Please ensure that you have converted to conll03 format.')\n",
    "      logger.error('Label {} not in valid label set {}.'.format(label,sanity_labels))\n",
    "      logger.error('Please ensure that you have converted to conll03 format.')\n",
    "      sys.exit(-1)\n",
    "\n",
    "  # Print a summary evaluation report.\n",
    "  cr = classification_report(gold_labels, predicted_labels, labels=label_names)\n",
    "  print('\\n{}\\n'.format(cr))\n",
    "  logger.info('\\n{}\\n'.format(cr))\n",
    "  # Print the full confusion matrix\n",
    "  cm = confusion_matrix(gold_labels, predicted_labels, labels=label_names)\n",
    "  print('\\n{}\\n'.format(cm))\n",
    "  logger.info('\\n{}\\n'.format(cm))\n",
    "\n",
    "  # One Hot encode multiclass predictions so that a one versus all AUC can be\n",
    "  # computed.\n",
    "  label_binarizer = LabelBinarizer().fit(gold_labels)\n",
    "  y_onehot_test = label_binarizer.transform(predicted_labels)\n",
    "  x_onehot_true = label_binarizer.transform(gold_labels)\n",
    "  micro_roc_auc_ovr = roc_auc_score(y_onehot_test, x_onehot_true,\n",
    "                                    multi_class=\"ovr\", average=\"micro\")\n",
    "  print(f\"\\nMicro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\\n\")\n",
    "  logger.info(f\"\\nMicro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\\n\")\n",
    "  return\n",
    "\n",
    "# Utility function to write a gzip conllu file.\n",
    "# Example:\n",
    "# output_text = ne_spacy_weblg('input.txt.gz')\n",
    "# write_conllu_gzip('spacy-weblg.conllu.gz, output_text)                \n",
    "@nlptimer\n",
    "def write_conllu_gzip(output_file, output_text):\n",
    "  with gzip.open(output_file, 'wt') as fh:\n",
    "      fh.write(output_text)\n",
    "  return\n",
    "\n",
    "# Utility function to get number of lines for tqdm progress bar.\n",
    "# Complete file line length should be 249212.\n",
    "@nlptimer\n",
    "def get_file_len(input_file):\n",
    "  with gzip.open(input_file, 'rt') as fh:\n",
    "    num_lines = sum(1 for line in fh)\n",
    "  return num_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a template function that demonstrate how to walk the input file, which has one tokenized sentence per line, and output a tsv file with one parsed token per line. Each output sentence is separated by a single blank line. This code does not show you how to configure the parsing pipeline for each of the three libraries -- only to demonstrate the parser's output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_file_len executed in: 0.0003497600555419922 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 45003.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tThe\tA_LABEL\n",
      "1\tresulting\tA_LABEL\n",
      "2\timplementation\tA_LABEL\n",
      "3\tof\tA_LABEL\n",
      "4\tthe\tA_LABEL\n",
      "5\tIA-64\tA_LABEL\n",
      "6\t64-bit\tA_LABEL\n",
      "7\tarchitecture\tA_LABEL\n",
      "8\twas\tA_LABEL\n",
      "9\tthe\tA_LABEL\n",
      "10\tItanium\tA_LABEL\n",
      "11\t,\tA_LABEL\n",
      "12\tfinally\tA_LABEL\n",
      "13\tintroduced\tA_LABEL\n",
      "14\tin\tA_LABEL\n",
      "15\tJune\tA_LABEL\n",
      "16\t2001\tA_LABEL\n",
      "17\t.\tA_LABEL\n",
      "\n",
      "0\tRaytheon\tA_LABEL\n",
      "1\tis\tA_LABEL\n",
      "2\talso\tA_LABEL\n",
      "3\tworking\tA_LABEL\n",
      "4\twith\tA_LABEL\n",
      "5\tthe\tA_LABEL\n",
      "6\tMissile\tA_LABEL\n",
      "7\tDefense\tA_LABEL\n",
      "8\tAgency\tA_LABEL\n",
      "9\tto\tA_LABEL\n",
      "10\tdevelop\tA_LABEL\n",
      "11\tthe\tA_LABEL\n",
      "12\tNetwork\tA_LABEL\n",
      "13\tCentric\tA_LABEL\n",
      "14\tAirborne\tA_LABEL\n",
      "15\tDefense\tA_LABEL\n",
      "16\tElement\tA_LABEL\n",
      "17\t(\tA_LABEL\n",
      "18\tNCADE\tA_LABEL\n",
      "19\t)\tA_LABEL\n",
      "20\t,\tA_LABEL\n",
      "21\tan\tA_LABEL\n",
      "22\tanti-ballistic\tA_LABEL\n",
      "23\tmissile\tA_LABEL\n",
      "24\tderived\tA_LABEL\n",
      "25\tfrom\tA_LABEL\n",
      "26\tthe\tA_LABEL\n",
      "27\tAIM-120\tA_LABEL\n",
      "28\t.\tA_LABEL\n",
      "\n",
      "0\tMuch\tA_LABEL\n",
      "1\tof\tA_LABEL\n",
      "2\this\tA_LABEL\n",
      "3\twork\tA_LABEL\n",
      "4\tat\tA_LABEL\n",
      "5\tthe\tA_LABEL\n",
      "6\tpatent\tA_LABEL\n",
      "7\toffice\tA_LABEL\n",
      "8\trelated\tA_LABEL\n",
      "9\tto\tA_LABEL\n",
      "10\tquestions\tA_LABEL\n",
      "11\tabout\tA_LABEL\n",
      "12\ttransmission\tA_LABEL\n",
      "13\tof\tA_LABEL\n",
      "14\telectric\tA_LABEL\n",
      "15\tsignals\tA_LABEL\n",
      "16\tand\tA_LABEL\n",
      "17\telectrical-mechanical\tA_LABEL\n",
      "18\tsynchronization\tA_LABEL\n",
      "19\tof\tA_LABEL\n",
      "20\ttime\tA_LABEL\n",
      "21\t,\tA_LABEL\n",
      "22\ttwo\tA_LABEL\n",
      "23\ttechnical\tA_LABEL\n",
      "24\tproblems\tA_LABEL\n",
      "25\tthat\tA_LABEL\n",
      "26\tshow\tA_LABEL\n",
      "27\tup\tA_LABEL\n",
      "28\tconspicuously\tA_LABEL\n",
      "29\tin\tA_LABEL\n",
      "30\tthe\tA_LABEL\n",
      "31\tthought\tA_LABEL\n",
      "32\texperiments\tA_LABEL\n",
      "33\tthat\tA_LABEL\n",
      "34\teventually\tA_LABEL\n",
      "35\tled\tA_LABEL\n",
      "36\tEinstein\tA_LABEL\n",
      "37\tto\tA_LABEL\n",
      "38\this\tA_LABEL\n",
      "39\tradical\tA_LABEL\n",
      "40\tconclusions\tA_LABEL\n",
      "41\tabout\tA_LABEL\n",
      "42\tthe\tA_LABEL\n",
      "43\tnature\tA_LABEL\n",
      "44\tof\tA_LABEL\n",
      "45\tlight\tA_LABEL\n",
      "46\tand\tA_LABEL\n",
      "47\tthe\tA_LABEL\n",
      "48\tfundamental\tA_LABEL\n",
      "49\tconnection\tA_LABEL\n",
      "50\tbetween\tA_LABEL\n",
      "51\tspace\tA_LABEL\n",
      "52\tand\tA_LABEL\n",
      "53\ttime\tA_LABEL\n",
      "54\t.\tA_LABEL\n",
      "\n",
      "0\tIn\tA_LABEL\n",
      "1\tthe\tA_LABEL\n",
      "2\tlate\tA_LABEL\n",
      "3\t1980\tA_LABEL\n",
      "4\ts\tA_LABEL\n",
      "5\tand\tA_LABEL\n",
      "6\tearly\tA_LABEL\n",
      "7\t1990\tA_LABEL\n",
      "8\ts\tA_LABEL\n",
      "9\t,\tA_LABEL\n",
      "10\tshe\tA_LABEL\n",
      "11\tgained\tA_LABEL\n",
      "12\tfame\tA_LABEL\n",
      "13\tthrough\tA_LABEL\n",
      "14\ther\tA_LABEL\n",
      "15\trole\tA_LABEL\n",
      "16\tin\tA_LABEL\n",
      "17\t\"\tA_LABEL\n",
      "18\tRoseanne\tA_LABEL\n",
      "19\t\"\tA_LABEL\n",
      "20\tand\tA_LABEL\n",
      "21\tother\tA_LABEL\n",
      "22\tperformances\tA_LABEL\n",
      "23\t.\tA_LABEL\n",
      "\n",
      "0\tFor\tA_LABEL\n",
      "1\texample\tA_LABEL\n",
      "2\t,\tA_LABEL\n",
      "3\tthe\tA_LABEL\n",
      "4\tRutherford\tA_LABEL\n",
      "5\tcross-section\tA_LABEL\n",
      "6\tis\tA_LABEL\n",
      "7\ta\tA_LABEL\n",
      "8\tmeasure\tA_LABEL\n",
      "9\tof\tA_LABEL\n",
      "10\tprobability\tA_LABEL\n",
      "11\tthat\tA_LABEL\n",
      "12\tan\tA_LABEL\n",
      "13\talpha-particle\tA_LABEL\n",
      "14\twill\tA_LABEL\n",
      "15\tbe\tA_LABEL\n",
      "16\tdeflected\tA_LABEL\n",
      "17\tby\tA_LABEL\n",
      "18\ta\tA_LABEL\n",
      "19\tgiven\tA_LABEL\n",
      "20\tangle\tA_LABEL\n",
      "21\tduring\tA_LABEL\n",
      "22\ta\tA_LABEL\n",
      "23\tcollision\tA_LABEL\n",
      "24\twith\tA_LABEL\n",
      "25\tan\tA_LABEL\n",
      "26\tatomic\tA_LABEL\n",
      "27\tnucleus\tA_LABEL\n",
      "28\t.\tA_LABEL\n",
      "\n",
      "0\tWith\tA_LABEL\n",
      "1\tthe\tA_LABEL\n",
      "2\tclub\tA_LABEL\n",
      "3\tin\tA_LABEL\n",
      "4\tfourth\tA_LABEL\n",
      "5\tplace\tA_LABEL\n",
      "6\tmidway\tA_LABEL\n",
      "7\tthrough\tA_LABEL\n",
      "8\tthe\tA_LABEL\n",
      "9\t1988\tA_LABEL\n",
      "10\tseason\tA_LABEL\n",
      "11\tat\tA_LABEL\n",
      "12\tthe\tA_LABEL\n",
      "13\tAll-Star\tA_LABEL\n",
      "14\tbreak\tA_LABEL\n",
      "15\t,\tA_LABEL\n",
      "16\tmanager\tA_LABEL\n",
      "17\tJohn\tA_LABEL\n",
      "18\tMcNamara\tA_LABEL\n",
      "19\twas\tA_LABEL\n",
      "20\tfired\tA_LABEL\n",
      "21\tand\tA_LABEL\n",
      "22\treplaced\tA_LABEL\n",
      "23\tby\tA_LABEL\n",
      "24\tJoe\tA_LABEL\n",
      "25\tMorgan\tA_LABEL\n",
      "26\ton\tA_LABEL\n",
      "27\tJuly\tA_LABEL\n",
      "28\t15\tA_LABEL\n",
      "29\t.\tA_LABEL\n",
      "\n",
      "0\tAs\tA_LABEL\n",
      "1\tthe\tA_LABEL\n",
      "2\tband\tA_LABEL\n",
      "3\t's\tA_LABEL\n",
      "4\tprimary\tA_LABEL\n",
      "5\tsongwriter\tA_LABEL\n",
      "6\t,\tA_LABEL\n",
      "7\tTony\tA_LABEL\n",
      "8\tIommi\tA_LABEL\n",
      "9\twrote\tA_LABEL\n",
      "10\tthe\tA_LABEL\n",
      "11\tmajority\tA_LABEL\n",
      "12\tof\tA_LABEL\n",
      "13\tBlack\tA_LABEL\n",
      "14\tSabbath\tA_LABEL\n",
      "15\t's\tA_LABEL\n",
      "16\tmusic\tA_LABEL\n",
      "17\t,\tA_LABEL\n",
      "18\twhile\tA_LABEL\n",
      "19\tOsbourne\tA_LABEL\n",
      "20\twould\tA_LABEL\n",
      "21\twrite\tA_LABEL\n",
      "22\tvocal\tA_LABEL\n",
      "23\tmelodies\tA_LABEL\n",
      "24\t,\tA_LABEL\n",
      "25\tand\tA_LABEL\n",
      "26\tbassist\tA_LABEL\n",
      "27\tGeezer\tA_LABEL\n",
      "28\tButler\tA_LABEL\n",
      "29\twould\tA_LABEL\n",
      "30\twrite\tA_LABEL\n",
      "31\tlyrics\tA_LABEL\n",
      "32\t.\tA_LABEL\n",
      "\n",
      "0\tThompson\tA_LABEL\n",
      "1\tplays\tA_LABEL\n",
      "2\ta\tA_LABEL\n",
      "3\tfictional\tA_LABEL\n",
      "4\tcharacter\tA_LABEL\n",
      "5\ton\tA_LABEL\n",
      "6\tthe\tA_LABEL\n",
      "7\ttelevision\tA_LABEL\n",
      "8\tseries\tA_LABEL\n",
      "9\tLaw\tA_LABEL\n",
      "10\tand\tA_LABEL\n",
      "11\tOrder\tA_LABEL\n",
      "12\t.\tA_LABEL\n",
      "\n",
      "0\tIn\tA_LABEL\n",
      "1\t1637\tA_LABEL\n",
      "2\tJohn\tA_LABEL\n",
      "3\tBastwick\tA_LABEL\n",
      "4\t,\tA_LABEL\n",
      "5\tHenry\tA_LABEL\n",
      "6\tBurton\tA_LABEL\n",
      "7\t,\tA_LABEL\n",
      "8\tand\tA_LABEL\n",
      "9\tWilliam\tA_LABEL\n",
      "10\tPrynne\tA_LABEL\n",
      "11\thad\tA_LABEL\n",
      "12\ttheir\tA_LABEL\n",
      "13\tears\tA_LABEL\n",
      "14\tcut\tA_LABEL\n",
      "15\toff\tA_LABEL\n",
      "16\tfor\tA_LABEL\n",
      "17\twriting\tA_LABEL\n",
      "18\tpamphlets\tA_LABEL\n",
      "19\tattacking\tA_LABEL\n",
      "20\tLaud\tA_LABEL\n",
      "21\t's\tA_LABEL\n",
      "22\tviews\tA_LABEL\n",
      "23\t--\tA_LABEL\n",
      "24\ta\tA_LABEL\n",
      "25\trare\tA_LABEL\n",
      "26\tpenalty\tA_LABEL\n",
      "27\tfor\tA_LABEL\n",
      "28\tgentlemen\tA_LABEL\n",
      "29\t,\tA_LABEL\n",
      "30\tand\tA_LABEL\n",
      "31\tone\tA_LABEL\n",
      "32\tthat\tA_LABEL\n",
      "33\taroused\tA_LABEL\n",
      "34\tanger\tA_LABEL\n",
      "35\t.\tA_LABEL\n",
      "\n",
      "0\tHe\tA_LABEL\n",
      "1\tthen\tA_LABEL\n",
      "2\tsigned\tA_LABEL\n",
      "3\ta\tA_LABEL\n",
      "4\tminor\tA_LABEL\n",
      "5\tleague\tA_LABEL\n",
      "6\tdeal\tA_LABEL\n",
      "7\twith\tA_LABEL\n",
      "8\tthe\tA_LABEL\n",
      "9\tWashington\tA_LABEL\n",
      "10\tNationals\tA_LABEL\n",
      "11\ton\tA_LABEL\n",
      "12\tMay\tA_LABEL\n",
      "13\t3\tA_LABEL\n",
      "14\t.\tA_LABEL\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def example_parser(input_file):\n",
    "  output_file = []\n",
    "  num_lines = get_file_len(input_file)\n",
    "  with gzip.open(input_file, 'rt') as fh:\n",
    "    for ln in tqdm(fh, total=num_lines):\n",
    "      tokens = ln.strip().split()\n",
    "      for i,t in enumerate(tokens):\n",
    "        output_file.append('{}\\t{}\\t{}'.format(i, t, 'A_LABEL'))\n",
    "      output_file.append('')\n",
    "  return '\\n'.join(output_file)\n",
    "\n",
    "# Run on the sample file.\n",
    "output_text = example_parser('sample.txt.gz')\n",
    "print (output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza NER Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanza is a collection of accurate and efficient tools for many human languages in one place. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing. Stanza is a Python natural language analysis package. It contains tools, which can be used in a pipeline, to convert a string containing human language text into lists of sentences and words, to generate base forms of those words, their parts of speech and morphological features, to give a syntactic structure dependency parse, and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, using the Universal Dependencies formalism.\n",
    "\n",
    "Native Python implementation requiring minimal efforts to set up; Full neural network pipeline for robust text analytics, including tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging, dependency parsing, and named entity recognition; Pretrained neural models supporting 66 (human) languages; A stable, officially maintained Python interface to CoreNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nlptimer\n",
    "def ne_stanza_tagger(input_file):\n",
    "    output_file = []\n",
    "    num_lines = get_file_len(input_file)\n",
    "    nlp = stanza.Pipeline(lang='en', processors={'ner': 'conll03_charlm'}, tokenize_pretokenized=True, use_gpu=True)\n",
    "    with gzip.open(input_file, 'rt') as fh:\n",
    "        for ln in tqdm(fh, total=num_lines):\n",
    "            doc = nlp (ln)\n",
    "            for sent in doc.sentences:\n",
    "                for i, t in enumerate(sent.tokens):\n",
    "                    output_file.append('{}\\t{}\\t{}'.format(i, t.text, t.ner))\n",
    "            output_file.append('')\n",
    "    return '\\n'.join(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run Stanza to tag named entities.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 01:58:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_file_len executed in: 0.15065765380859375 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7417c9f5af7046aa8ad8393fc7fecfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 01:58:33 INFO: Downloaded file to /home/nqmtien/stanza_resources/resources.json\n",
      "2024-08-21 01:58:34 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus_charlm      |\n",
      "| ner          | conll03_charlm      |\n",
      "======================================\n",
      "\n",
      "2024-08-21 01:58:34 INFO: Using device: cuda\n",
      "2024-08-21 01:58:34 INFO: Loading: tokenize\n",
      "2024-08-21 01:58:34 INFO: Loading: mwt\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/mwt/trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-21 01:58:34 INFO: Loading: pos\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-21 01:58:35 INFO: Loading: lemma\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-21 01:58:35 INFO: Loading: constituency\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-21 01:58:35 INFO: Loading: depparse\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-21 01:58:35 INFO: Loading: sentiment\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-21 01:58:35 INFO: Loading: ner\n",
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-21 01:58:35 INFO: Done loading processors!\n",
      " 14%|██████████▎                                                             | 35839/249212 [8:00:17<8:47:54,  6.74it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 249212/249212 [20:06:02<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ne_stanza_tagger executed in: 72365.4467689991 seconds\n",
      "\n",
      "\n",
      "Writing Stanza output\n",
      "\n",
      "\n",
      "write_conllu_gzip executed in: 8.345110654830933 seconds\n",
      "\n",
      "\n",
      "Evaluate the Stanza model tags\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PER       0.91      0.93      0.92    212736\n",
      "        MISC       0.81      0.72      0.76    178313\n",
      "         LOC       0.86      0.86      0.86    197732\n",
      "         ORG       0.70      0.77      0.73    146036\n",
      "\n",
      "    accuracy                           0.83    734817\n",
      "   macro avg       0.82      0.82      0.82    734817\n",
      "weighted avg       0.83      0.83      0.83    734817\n",
      "\n",
      "\n",
      "\n",
      "[[197729   5484   3837   5686]\n",
      " [ 11186 128684  10139  28304]\n",
      " [  4173   8711 169837  15011]\n",
      " [  4544  16477  12557 112458]]\n",
      "\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.89\n",
      "\n",
      "\n",
      "score_conllu_file executed in: 10.344003677368164 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the stanza parser and generate a compressed conllu output of the results.\n",
    "# Toggle between the commented out sample / full dataset.\n",
    "\n",
    "print('\\nRun Stanza to tag named entities.\\n')\n",
    "logger.info('\\nRun Stanza to tag named entities.\\n')\n",
    "#output_text = ne_stanza_tagger('sample.txt.gz')\n",
    "output_text = ne_stanza_tagger('input.txt.gz')\n",
    "\n",
    "print('\\nWriting Stanza output\\n')\n",
    "logger.info('\\nWriting Stanz output\\n')\n",
    "write_conllu_gzip('stanza.conllu.gz', output_text)\n",
    "\n",
    "print('\\nEvaluate the Stanza model tags\\n')\n",
    "logger.info('\\nEvaluate the Stanza model tags\\n')\n",
    "# score_conllu_file('stanza.conllu.gz', 'sample.conllu.gz')\n",
    "score_conllu_file('stanza.conllu.gz', 'input.conllu.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK NER Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Overview of NLTK\n",
    "\n",
    "NLTK stands for the Natural Language Toolkit and is written by two eminent computational linguists, Steven Bird (Senior Research Associate of the LDC and professor at the University of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University). NTLK provides a combination of natural language corpora, lexical resources, and example grammars with language processing algorithms, methodologies and demonstrations for a very pythonic \"batteries included\" view of Natural Language Processing.   \n",
    "\n",
    "As such, NLTK is perfect for researh driven (hypothesis driven) workflows for agile data science. Its suite of libraries includes:\n",
    "\n",
    "- tokenization, stemming, and tagging\n",
    "- chunking and parsing\n",
    "- language modeling\n",
    "- classification and clustering\n",
    "- logical semantics\n",
    "\n",
    "NLTK is a useful pedagogical resource for learning NLP with Python and serves as a starting place for producing production grade code that requires natural language analysis. It is also important to understand what NLTK is _not_:\n",
    "\n",
    "- Production ready out of the box\n",
    "- Lightweight\n",
    "- Generally applicable\n",
    "- Magic\n",
    "\n",
    "NLTK provides a variety of tools that can be used to explore the linguistic domain but is not a lightweight dependency that can be easily included in other workflows, especially those that require unit and integration testing or other build processes. This stems from the fact that NLTK includes a lot of added code but also a rich and complete library of corpora that power the built-in algorithms. \n",
    "\n",
    "### The Good parts of NLTK\n",
    "\n",
    "- Preprocessing\n",
    "    - segmentation\n",
    "    - tokenization\n",
    "    - PoS tagging\n",
    "- Word level processing\n",
    "    - WordNet\n",
    "    - Lemmatization\n",
    "    - Stemming\n",
    "    - NGrams\n",
    "- Utilities\n",
    "    - Tree\n",
    "    - FreqDist\n",
    "    - ConditionalFreqDist\n",
    "    - Streaming CorpusReaders\n",
    "- Classification\n",
    "    - Maximum Entropy\n",
    "    - Naive Bayes\n",
    "    - Decision Tree\n",
    "- Chunking\n",
    "- Named Entity Recognition\n",
    "- Parsers Galore!\n",
    "\n",
    "### The Bad parts of NLTK\n",
    "\n",
    "- Syntactic Parsing\n",
    "\n",
    "    - No included grammar (not a black box)\n",
    "    - No Feature/Dependency Parsing\n",
    "    - No included feature grammar\n",
    "\n",
    "- The sem package\n",
    "    \n",
    "    - Toy only (lambda-calculus & first order logic)\n",
    "\n",
    "- Lots of extra stuff (heavyweight dependency)\n",
    "\n",
    "    - papers, chat programs, alignments, etc.\n",
    "\n",
    "## Named Entities\n",
    "\n",
    "NLTK has an excellent MaxEnt backed Named Entity Recognizer that is trained on the Penn Treebank. You can also retrain the chunker if you'd like - the code is very readable to extend it with a Gazette or otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NLTK uses a newer tagset than the gold labels, so they are automatically converted for you in the runtime code using a remapping function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nlptimer\n",
    "def ne_nltk_tagger(input_file):\n",
    "    output_file = []\n",
    "    num_lines = get_file_len(input_file)\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    with gzip.open(input_file, 'rt') as fh:\n",
    "        for ln in tqdm(fh, total=num_lines):\n",
    "            tokens = tokenizer.tokenize(ln)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            chunks = nltk.chunk.ne_chunk(pos_tags)\n",
    "            bio_tags = tree2conlltags(chunks)\n",
    "            i = 0\n",
    "            for w, t, n in bio_tags:\n",
    "                output_file.append('{}\\t{}\\t{}'.format(i, w, n))\n",
    "                i += 1\n",
    "            output_file.append('')\n",
    "    return('\\n'.join(output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run NLTK to tag named entities.\n",
      "\n",
      "\n",
      "get_file_len executed in: 0.26224255561828613 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 249212/249212 [11:40<00:00, 355.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ne_nltk_tagger executed in: 701.5920491218567 seconds\n",
      "\n",
      "\n",
      "Writing NLTK output\n",
      "\n",
      "\n",
      "write_conllu_gzip executed in: 9.506311655044556 seconds\n",
      "\n",
      "\n",
      "Convert NLTK output format to conll03.\n",
      "\n",
      "\n",
      "convert_nltk_to_conll executed in: 13.825253963470459 seconds\n",
      "\n",
      "\n",
      "Evaluate the NLTK model tags\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PER       0.64      0.84      0.73    198101\n",
      "        MISC       0.15      0.01      0.01    109178\n",
      "         LOC       0.58      0.55      0.56    179040\n",
      "         ORG       0.42      0.59      0.49    120924\n",
      "\n",
      "    accuracy                           0.56    607243\n",
      "   macro avg       0.45      0.50      0.45    607243\n",
      "weighted avg       0.49      0.56      0.50    607243\n",
      "\n",
      "\n",
      "\n",
      "[[166898    123  19253  11827]\n",
      " [ 29876    700  33601  45001]\n",
      " [ 35608   2518  98782  42132]\n",
      " [ 28265   1377  19736  71546]]\n",
      "\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.70\n",
      "\n",
      "\n",
      "score_conllu_file executed in: 11.723029851913452 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the NLTK parser  and generate a compressed conllu output of the results.\n",
    "# Toggle between the sample and full collections as you develop your code.\n",
    "\n",
    "print('\\nRun NLTK to tag named entities.\\n')\n",
    "logger.info('\\nRun NLTK to tag named entities.\\n')\n",
    "#output_text = ne_nltk_tagger('sample.txt.gz')\n",
    "output_text = ne_nltk_tagger('input.txt.gz')\n",
    "\n",
    "print('\\nWriting NLTK output\\n')\n",
    "logger.info('\\nWriting NLTK output\\n')\n",
    "write_conllu_gzip('nltk.gz', output_text)\n",
    "\n",
    "# Convert Spacy OntoNotes labels to Conll03\n",
    "print('\\nConvert NLTK output format to conll03.\\n')\n",
    "logger.info('\\nConvert NLTK output format to conll03.\\n')\n",
    "convert_nltk_to_conll('nltk.gz', 'nltk.conllu.gz')\n",
    "\n",
    "print('\\nEvaluate the NLTK model tags\\n')\n",
    "logger.info('\\nEvaluate the NLTK model tags\\n')\n",
    "# score_conllu_file('nltk.conllu.gz', 'sample.conllu.gz')\n",
    "score_conllu_file('nltk.conllu.gz', 'input.conllu.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy NER Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with Machine Learning using spaCy\n",
    "[spaCy](https://spacy.io) is a free and open-source package that can be used to perform automated named entity recognition.\n",
    "\n",
    "spaCy uses a form of machine learning called **convolutional neural networks** (CNNs) for named entity recognition. The nature of these neural networks is out of scope for this workshop, but in general terms a CNN produces a **statistical model** that is used to **predict** what are the most likely named entities in a text. This type of machine learning is described as being **supervised**, which means it must be trained on data that has been correctly labelled with named entities before it can do automated labelling on data it's never seen before.\n",
    "\n",
    "As a statistical technique, the predictions might be wrong; in fact, they often are, and it's usually necessary to re-train and adjust the model until its predictions are better. We will cover training a model in the following notebooks.\n",
    "\n",
    "Fortunately, we don't need to start completely from scratch because spaCy provides a range of [pre-trained language models](https://spacy.io/usage/models#languages) for modern languages such as English, Spanish, French and German. For other languages you can train your own and use them with spaCy or re-train one of the existing models. In this notebook, we will be using four different English models for comparison.\n",
    "\n",
    "The English models that spaCy offers have been trained on the [OntoNotes corpus](https://catalog.ldc.upenn.edu/LDC2013T19). These models can predict the following entities 'out of the box':\n",
    "\n",
    "<table class=\"_59fbd182\"><thead><tr class=\"_8a68569b\"><th class=\"_2e8d2972\">Type</th><th class=\"_2e8d2972\">Description</th></tr></thead><tbody><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">PERSON</code></td><td class=\"_5c99da9a\">People, including fictional.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">NORP</code></td><td class=\"_5c99da9a\">Nationalities or religious or political groups.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">FAC</code></td><td class=\"_5c99da9a\">Buildings, airports, highways, bridges, etc.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">ORG</code></td><td class=\"_5c99da9a\">Companies, agencies, institutions, etc.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">GPE</code></td><td class=\"_5c99da9a\">Countries, cities, states.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">LOC</code></td><td class=\"_5c99da9a\">Non-GPE locations, mountain ranges, bodies of water.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">PRODUCT</code></td><td class=\"_5c99da9a\">Objects, vehicles, foods, etc. (Not services.)</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">EVENT</code></td><td class=\"_5c99da9a\">Named hurricanes, battles, wars, sports events, etc.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">WORK_OF_ART</code></td><td class=\"_5c99da9a\">Titles of books, songs, etc.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">LAW</code></td><td class=\"_5c99da9a\">Named documents made into laws.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">LANGUAGE</code></td><td class=\"_5c99da9a\">Any named language.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">DATE</code></td><td class=\"_5c99da9a\">Absolute or relative dates or periods.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">TIME</code></td><td class=\"_5c99da9a\">Times smaller than a day.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">PERCENT</code></td><td class=\"_5c99da9a\">Percentage, including ”%“.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">MONEY</code></td><td class=\"_5c99da9a\">Monetary values, including unit.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">QUANTITY</code></td><td class=\"_5c99da9a\">Measurements, as of weight or distance.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">ORDINAL</code></td><td class=\"_5c99da9a\">“first”, “second”, etc.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">CARDINAL</code></td><td class=\"_5c99da9a\">Numerals that do not fall under another type.</td></tr></tbody></table>\n",
    "\n",
    "Note that these do not align with the CONLL 2003 labeled data that we are using, which include only four different labels (PER, LOC, ORG, and MISC). For convenience, I have included a function that will automatically remap the spaCy named entity labels to the gold labels we have available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy is arguably the most popular library for information extraction related tasks, and it includes several different models. The most difficult part is to get white space tokenization to work correctly, and so I have included a WhiteSpaceTokenizer class that can be used in the spaCy pipeline as it is non-trivial to disable all of the special tokenisation rules built into its tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkeypatch Spacy tokenizer to a whitespace only tokenizer.\n",
    "\n",
    "class SpWhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        spaces = [True] * len(words)\n",
    "        # Avoid zero-length tokens\n",
    "        for i, word in enumerate(words):\n",
    "            if word == \"\":\n",
    "                words[i] = \" \"\n",
    "                spaces[i] = False\n",
    "        # Remove the final trailing space\n",
    "        if words[-1] == \" \":\n",
    "            words = words[0:-1]\n",
    "            spaces = spaces[0:-1]\n",
    "        else:\n",
    "           spaces[-1] = False\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nlptimer\n",
    "def ne_spacy_tagger(input_file, model_name):\n",
    "    nlp_en = spacy.load(model_name)\n",
    "    output_lines = []\n",
    "    nlp_en.tokenizer = SpWhitespaceTokenizer(nlp_en.vocab)\n",
    "    num_lines = get_file_len(input_file)\n",
    "    with gzip.open(input_file, 'rt') as fh:\n",
    "        for ln in tqdm(fh, total=num_lines):\n",
    "            doc = nlp_en(ln)\n",
    "            for i, token in enumerate(doc):\n",
    "                ent_label = token.ent_iob_ + '-' + token.ent_type_ if token.ent_iob_ != 'O' else 'O'\n",
    "                if (token.text[-1] == '\\n'):\n",
    "                    output_lines.append('{}\\t{}\\t{}'.format(i, token.text[:-1], ent_label))\n",
    "                else:\n",
    "                    output_lines.append('{}\\t{}\\t{}'.format(i, token.text, ent_label))\n",
    "            output_lines.append('')\n",
    "    return ('\\n'.join(output_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run four different spacy models: en_core_web_sm, en_core_web_med, en_core_web_lg, and en_core_web_trf. Note the performance differences for each model choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run Spacy en_core_web_sm model to tag named entities.\n",
      "\n",
      "\n",
      "get_file_len executed in: 0.2169325351715088 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 249212/249212 [23:32<00:00, 176.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ne_spacy_tagger executed in: 1414.0299181938171 seconds\n",
      "\n",
      "\n",
      "Writing Spacy Web Small output\n",
      "\n",
      "\n",
      "write_conllu_gzip executed in: 7.8404929637908936 seconds\n",
      "\n",
      "\n",
      "Convert Spacy output format to conll03.\n",
      "\n",
      "\n",
      "convert_ontonotes_to_conll executed in: 13.31121826171875 seconds\n",
      "\n",
      "Evaluate Spacy en_core_web_sm model tags\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PER       0.79      0.79      0.79    197666\n",
      "        MISC       0.72      0.55      0.62    146701\n",
      "         LOC       0.80      0.67      0.73    184585\n",
      "         ORG       0.53      0.79      0.64    132417\n",
      "\n",
      "    accuracy                           0.70    661369\n",
      "   macro avg       0.71      0.70      0.69    661369\n",
      "weighted avg       0.73      0.70      0.71    661369\n",
      "\n",
      "\n",
      "\n",
      "[[155465   6695   9257  26249]\n",
      " [ 15985  81125  11824  37767]\n",
      " [ 15307  16703 123792  28783]\n",
      " [  9536   8500   9534 104847]]\n",
      "\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.80\n",
      "\n",
      "\n",
      "score_conllu_file executed in: 13.042055130004883 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Spacy Web Small and generate a compressed conllu output of the results.\n",
    "\n",
    "print('\\nRun Spacy en_core_web_sm model to tag named entities.\\n')\n",
    "logger.info('\\nRun Spacy en_core_web_sm model to tag named entities.\\n')\n",
    "# output_text = ne_spacy_tagger('sample.txt.gz', 'en_core_web_sm')\n",
    "output_text = ne_spacy_tagger('input.txt.gz','en_core_web_sm')\n",
    "\n",
    "print('\\nWriting Spacy Web Small output\\n')\n",
    "logger.info('\\nWriting Spacy Web Small output\\n')\n",
    "write_conllu_gzip('spacy-websm.gz', output_text)\n",
    "\n",
    "# Convert Spacy OntoNotes labels to Conll03.\n",
    "print('\\nConvert Spacy output format to conll03.\\n')\n",
    "logger.info('\\nConvert Spacy output format to conll03.\\n')\n",
    "convert_ontonotes_to_conll('spacy-websm.gz', 'spacy-websm.conllu.gz')\n",
    "\n",
    "print('Evaluate Spacy en_core_web_sm model tags\\n')\n",
    "logger.info('Evaluate Spacy en_core_web_sm model tags\\n')\n",
    "# score_conllu_file('spacy-websm.conllu.gz', 'sample.conllu.gz')\n",
    "score_conllu_file('spacy-websm.conllu.gz', 'input.conllu.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run Spacy en_core_web_md model to tag named entities.\n",
      "\n",
      "\n",
      "get_file_len executed in: 0.201155424118042 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 249212/249212 [20:35<00:00, 201.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ne_spacy_tagger executed in: 1237.2120561599731 seconds\n",
      "\n",
      "\n",
      "Writing Spacy Web Medium output\n",
      "\n",
      "\n",
      "write_conllu_gzip executed in: 7.496793270111084 seconds\n",
      "\n",
      "\n",
      "Convert Spacy output format to conll03.\n",
      "\n",
      "\n",
      "convert_ontonotes_to_conll executed in: 11.162131786346436 seconds\n",
      "\n",
      "\n",
      "Evaluate Spacy en_core_web_md model tags\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PER       0.86      0.86      0.86    204155\n",
      "        MISC       0.73      0.62      0.67    147488\n",
      "         LOC       0.87      0.72      0.79    187873\n",
      "         ORG       0.58      0.82      0.68    135281\n",
      "\n",
      "    accuracy                           0.76    674797\n",
      "   macro avg       0.76      0.76      0.75    674797\n",
      "weighted avg       0.78      0.76      0.76    674797\n",
      "\n",
      "\n",
      "\n",
      "[[174684   5455   3737  20279]\n",
      " [ 13218  91848   7499  34923]\n",
      " [  9404  19234 135137  24098]\n",
      " [  6260   8577   8888 111556]]\n",
      "\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.84\n",
      "\n",
      "\n",
      "score_conllu_file executed in: 9.501359701156616 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run spaCy web mediaum and generate a compressed conllu out of the results.\n",
    "\n",
    "print('\\nRun Spacy en_core_web_md model to tag named entities.\\n')\n",
    "logger.info('\\nRun Spacy en_core_web_md model to tag named entities.\\n')\n",
    "# output_text = ne_spacy_tagger('sample.txt.gz', 'en_core_web_md')\n",
    "output_text = ne_spacy_tagger('input.txt.gz','en_core_web_md')\n",
    "\n",
    "print('\\nWriting Spacy Web Medium output\\n')\n",
    "logger.info('\\nWriting Spacy Web Medium output\\n')\n",
    "write_conllu_gzip('spacy-webmd.gz', output_text)\n",
    "\n",
    "# Convert Spacy OntoNotes labels to Conll03\n",
    "print('\\nConvert Spacy output format to conll03.\\n')\n",
    "logger.info('\\nConvert Spacy output format to conll03.\\n')\n",
    "convert_ontonotes_to_conll('spacy-webmd.gz', 'spacy-webmd.conllu.gz')\n",
    "\n",
    "print('\\nEvaluate Spacy en_core_web_md model tags\\n')\n",
    "logger.info('\\nEvaluate Spacy en_core_web_md model tags\\n')\n",
    "# score_conllu_file('spacy-webmd.conllu.gz', 'sample.conllu.gz')\n",
    "score_conllu_file('spacy-webmd.conllu.gz', 'input.conllu.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run Spacy en_core_web_lg model to tag named entities.\n",
      "\n",
      "\n",
      "get_file_len executed in: 0.195420503616333 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 249212/249212 [15:46<00:00, 263.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ne_spacy_tagger executed in: 950.7010734081268 seconds\n",
      "\n",
      "\n",
      "Writing Spacy Web Large output\n",
      "\n",
      "\n",
      "write_conllu_gzip executed in: 7.406781435012817 seconds\n",
      "\n",
      "\n",
      "Convert Spacy output format to conll03.\n",
      "\n",
      "\n",
      "convert_ontonotes_to_conll executed in: 11.194310426712036 seconds\n",
      "\n",
      "\n",
      "Evaluate Spacy en_core_web_lg model tags\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PER       0.87      0.87      0.87    204780\n",
      "        MISC       0.74      0.64      0.69    146459\n",
      "         LOC       0.88      0.72      0.79    188144\n",
      "         ORG       0.60      0.83      0.70    135880\n",
      "\n",
      "    accuracy                           0.77    675263\n",
      "   macro avg       0.77      0.77      0.76    675263\n",
      "weighted avg       0.79      0.77      0.77    675263\n",
      "\n",
      "\n",
      "\n",
      "[[178224   5114   3514  17928]\n",
      " [ 12008  94001   7092  33358]\n",
      " [  8690  19990 135896  23568]\n",
      " [  5480   8460   8655 113285]]\n",
      "\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.85\n",
      "\n",
      "\n",
      "score_conllu_file executed in: 9.719312906265259 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run spaCy Web Large and generate a compressed conllu output of the results\n",
    "\n",
    "print('\\nRun Spacy en_core_web_lg model to tag named entities.\\n')\n",
    "logger.info('\\nRun Spacy en_core_web_lg model to tag named entities.\\n')\n",
    "# output_text = ne_spacy_tagger('sample.txt.gz', 'en_core_web_lg')\n",
    "output_text = ne_spacy_tagger('input.txt.gz','en_core_web_lg')\n",
    "\n",
    "print('\\nWriting Spacy Web Large output\\n')\n",
    "logger.info('\\nWriting Spacy Web Large output\\n')\n",
    "write_conllu_gzip('spacy-weblg.gz', output_text)\n",
    "\n",
    "# Convert Spacy OntoNotes labels to Conll03\n",
    "print('\\nConvert Spacy output format to conll03.\\n')\n",
    "logger.info('\\nConvert Spacy output format to conll03.\\n')\n",
    "convert_ontonotes_to_conll('spacy-weblg.gz', 'spacy-weblg.conllu.gz')\n",
    "\n",
    "print('\\nEvaluate Spacy en_core_web_lg model tags\\n')\n",
    "logger.info('\\nEvaluate Spacy en_core_web_lg model tags\\n')\n",
    "# score_conllu_file('spacy-weblg.conllu.gz', 'sample.conllu.gz')\n",
    "score_conllu_file('spacy-weblg.conllu.gz', 'input.conllu.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run Spacy en_core_web_trf model to tag named entities.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_file_len executed in: 0.14785313606262207 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/249212 [00:00<?, ?it/s]/home/nqmtien/miniconda3/envs/COMP4703Lab1/lib/python3.8/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 249212/249212 [2:11:23<00:00, 31.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ne_spacy_tagger executed in: 7888.370453834534 seconds\n",
      "\n",
      "\n",
      "Writing Spacy Web Transformer output\n",
      "\n",
      "\n",
      "write_conllu_gzip executed in: 7.727949619293213 seconds\n",
      "\n",
      "\n",
      "Convert Spacy output format to conll03.\n",
      "\n",
      "\n",
      "convert_ontonotes_to_conll executed in: 11.209738969802856 seconds\n",
      "\n",
      "\n",
      "Evaluate Spacy en_core_web_trf model tags\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PER       0.95      0.96      0.96    210067\n",
      "        MISC       0.77      0.86      0.81    171526\n",
      "         LOC       0.94      0.79      0.86    193847\n",
      "         ORG       0.83      0.89      0.86    144687\n",
      "\n",
      "    accuracy                           0.87    720127\n",
      "   macro avg       0.87      0.87      0.87    720127\n",
      "weighted avg       0.88      0.87      0.88    720127\n",
      "\n",
      "\n",
      "\n",
      "[[202641   3403   1257   2766]\n",
      " [  5608 146757   4148  15013]\n",
      " [  1557  30845 152447   8998]\n",
      " [  2658   9552   4295 128182]]\n",
      "\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.92\n",
      "\n",
      "\n",
      "score_conllu_file executed in: 10.417960166931152 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Spacy Web Transformer and generate a compressed conllu output of the results.\n",
    "\n",
    "print('\\nRun Spacy en_core_web_trf model to tag named entities.\\n')\n",
    "logger.info('\\nRun Spacy en_core_web_trf model to tag named entities.\\n')\n",
    "# output_text = ne_spacy_tagger('sample.txt.gz', 'en_core_web_trf')\n",
    "output_text = ne_spacy_tagger('input.txt.gz','en_core_web_trf')\n",
    "\n",
    "print('\\nWriting Spacy Web Transformer output\\n')\n",
    "logger.info('\\nWriting Spacy Web Transformer output\\n')\n",
    "write_conllu_gzip('spacy-webtrf.gz', output_text)\n",
    "\n",
    "# Convert Spacy OntoNotes labels to Conll03\n",
    "print('\\nConvert Spacy output format to conll03.\\n')\n",
    "logger.info('\\nConvert Spacy output format to conll03.\\n')\n",
    "convert_ontonotes_to_conll('spacy-webtrf.gz', 'spacy-webtrf.conllu.gz')\n",
    "\n",
    "print('\\nEvaluate Spacy en_core_web_trf model tags\\n')\n",
    "logger.info('\\nEvaluate Spacy en_core_web_trf model tags\\n')\n",
    "# score_conllu_file('spacy-webtrf.conllu.gz', 'sample.conllu.gz')\n",
    "score_conllu_file('spacy-webtrf.conllu.gz', 'input.conllu.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first table summarises the model performance based on F1 and AUC. Table 2 is a summary of the number of correct answers for each of the four tags. Table 3 is the number of True Negatives for each tag (the sum of values in each row, ignoring the True Positive Value, and Table 4 is the number of False Negatives for each tag (the sum of the values in each column, ignoring the True Positive Value).\n",
    "<br><br>For Table 1, mark the best model is marked by boldfacing the highest score for each of the six metrics. For Table 2, the model value is boldfaced for the the most correct predictions for each tag.\n",
    "<br><br> For the last two tables, the model with the fewest True Negatives/False Negatives is marked using boldface, and the model with the most True Negatives/False Negatives is marked using italics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>TABLE 1 - Evaluation Metrics</center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | F1(PER) | F1(MISC) | F1(LOC) | F1(ORG) | F1(Macro AVG) | OvA AUC |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Stanza | 0.92 | 0.76 | **0.86** | 0.73 | 0.82 | 0.89 |\n",
    "| NLTK | 0.73 | 0.01 | 0.56 | 0.49 | 0.45 | 0.70 |\n",
    "| spaCy-sm | 0.79 | 0.62 | 0.73 | 0.64 | 0.69 | 0.80 |\n",
    "| spaCy-md | 0.86 | 0.67 | 0.79 | 0.68 | 0.75 | 0.84 |\n",
    "| spaCy-lg | 0.87 | 0.69 | 0.79 | 0.70 | 0.76 | 0.85 |\n",
    "| spaCy-tr | **0.96** | **0.81** | **0.86** | **0.86** | **0.87** | **0.92** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Table 2 - True Positive Counts </center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| TAG | Stanza | NLTK | spaCy_sm | spaCy_md | spaCy_lg | spaCy_tr |\n",
    "|---|---|---|---|---|---|---|\n",
    "| PER | 197729 | 166898 | **155465** | 174684 | 178224 | *202641* |\n",
    "| MISC | 128684 | **700** | 81125 | 91848 | 94001 | *146757* |\n",
    "| LOC | *169837* | **98782** | 123792 | 135137 | 135896 | 152447 |\n",
    "| ORG | 112458 | **71546** | 104847 | 111556 | 113285 | *128182* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Table 3 - True Negative Counts </center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| TAG | Stanza | NLTK | spaCy-sm | spaCy-md | spaCy-lg | spaCy-tr |\n",
    "|---|---|---|---|---|---|---|\n",
    "| PER | *502178* | **315393** | 422875 | 441760 | 444305 | 500237 |\n",
    "| MISC | *525832* | 494047 | **482770** | 494043 | 495240 | 504801 |\n",
    "| LOC | 510552 | **355613** | 446169 | 466800 | 467858 | *516580* |\n",
    "| ORG | 539780 | **387359** | 436153 | 460216 | 464529 | *548663* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Table 4 - False Negative Counts </center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| TAG | Stanza | NLTK | spaCy-sm | spaCy-md | spaCy-lg | spaCy-tr |\n",
    "|---|---|---|---|---|---|---|\n",
    "| PER | 15007 | 31203 | *42201* | 29471 | 26556 | **7426** |\n",
    "| MISC | 49629 | *108478* | 65576 | 55640 | 52458 | **24769** |\n",
    "| LOC | **27895** | *80258* | 60793 | 52736 | 52248 | 41400 |\n",
    "| ORG | 33578 | *49378* | 27570 | 23725 | 22595 | **16505** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
